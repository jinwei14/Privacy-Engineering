{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2: Big Data Anonymisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, you will manipulate a synthetic CDR (call data record) phone metadata dataset, that closely mimics real-world data. As we know, people in such datasets are very unique, and knowing a few points of someone's trajectory is most of the time enough to find this person uniquely. Note that this dataset is purely synthetic, and the unicity numbers might not match the numbers shown in class.\n",
    "\n",
    "In these exercises, you will first compute the _unicity_ of people in the dataset, and see how this unicity decreases with the size of the population. You will then generalise the location and time of the points collected to see how it decreases unicity. Finally, you will perform a more sophisticated attack, that works even when the data is corrupted by noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "This week, we will not be using Pandas, as real-world CDR datasets are way too large to fit in a Pandas dataframe. Instead, we will be manipulating raw data using Python structures (namely, lists and dictionaries).\n",
    "\n",
    "**Dataset description**\n",
    "\n",
    "This dataset is composed of metadata: every time a person calls (or gets called by) another or send (or receives) a text message, the time and place is stored in the dataset. In our case, the data is stored as a large text file, with every line describing one event, as:\n",
    "\n",
    "`user_id antenna_id timestamp`\n",
    "\n",
    "The `antenna_id` represents the antenna that served the user. These antennas are numbered, and give location information for the user. An additional file gives the mapping from antenna number to geographical locations, with one antenna per line, and each line as:\n",
    "\n",
    "`antenna_id latitude longitude`\n",
    "\n",
    "The `timestamp` corresponds to the _hour_ since the beginning of the dataset's data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific dataset that you will use contains 50,000 people, over a period of _one week_ in a geography you should be rather familiar with.\n",
    "\n",
    "**Warning**: this notebook is quite _memory intensive_, and should take around 2 GB of memory when all exercises have been executed. If you have reasons to believe that this might be an issue, we also provide you with a smaller dataset (10,000 users), which shouldn't take more than 550 MB (that's like running two Facebook tabs on Google Chrome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Python data structure and data loading**\n",
    "\n",
    "For starters, we will represent the CDR as a list of points, and the geography using a `Numpy` array with $n_{antennas}$ lines and two columns (longitude and latitude).\n",
    "\n",
    "On a technical note: for simplicity we will use lat-long coordinates as Cartesian coordinates on the Euclidean plane. Note however that this method is not completely accurate, as [distances](https://en.wikipedia.org/wiki/Great-circle_distance) are not [preserved](http://jonisalonen.com/2014/computing-distance-between-coordinates-can-be-simple-and-fast). It works however as a first approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CDR = []\n",
    "\n",
    "# TODO: use `CDR_10k.txt` if you want to reduce memory usage\n",
    "with open('CDR_50k.txt', 'r') as cdrfile:\n",
    "    for line in cdrfile:\n",
    "        user_id, antenna_id, timestamp = map(int, line.split())\n",
    "        CDR.append((user_id, antenna_id, timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, store the file in a dictionary\n",
    "antennasdict = {}\n",
    "with open('uk_antennas.txt', 'r') as antennafile:\n",
    "    for line in antennafile:\n",
    "        antenna_id, lat, long = line.split()\n",
    "        antennasdict[int(antenna_id)] = (float(lat), float(long))\n",
    "\n",
    "# define the numpy array, with the correct size (maxkey + 1, as indexing starts at 0)\n",
    "antennas = np.zeros((max(antennasdict.keys())+1,2))\n",
    "\n",
    "# encode information as required\n",
    "for antenna_id in antennasdict.keys():\n",
    "    antennas[antenna_id,:] = antennasdict[antenna_id]\n",
    "\n",
    "# delete unused information\n",
    "del antennasdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Plotting the geography**\n",
    "\n",
    "Let's have a quick look at what the geographical data tells us about antenna distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.scatter(antennas[:,1], antennas[:,0]);\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: the circadian rythms**\n",
    "\n",
    "Humans are known for their great [behavioural regularity over time](https://en.wikipedia.org/wiki/Circadian_rhythm): we work and do most of our interactions during the day, while nights are much quieter. This can actually be observed in `CDR` data. Let's plot the number of interactions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = [r[2] for r in CDR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this histogram displays the circadian rythms of human life\n",
    "plt.hist(times, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 | Computing unicity\n",
    "\n",
    "In the first step of this exercise, you will create a function to estimate the unicity of a dataset. Recall that unicity for $p$ points $\\varepsilon_p$ _estimates_ the fraction of users that are unique for $p$ random points.\n",
    "\n",
    "**Algorithm.**\n",
    "The procedure we will use to compute unicity is as follows:\n",
    "1. Select a random sample of the population $u_1, \\dots, u_m$.\n",
    "2. For each user $u_i$ in this sample, select $k$ points at random: $(p_{i,1}, \\dots, p_{i,k})$.\n",
    "3. Compute, for each user $u_i$, whether he is unique according to $(p_{i,1}, \\dots, p_{i,k})$ over the _full_ dataset.\n",
    "\n",
    "**Data structures.**\n",
    "The algorithm requires to be able to quickly access all of a user's points, as well as check easily whether a set of points is unique in a population. The data structures that we propose to use in this case are a couple of _dictionaries_ named `u2p` (_user to point_) and `p2u` (_point to user_):\n",
    "- `u2p` maps each user ID to the _list_ of its points: `[(antenna1, time1), ..., (antennaL, timeL)]`\n",
    "- `p2u` maps each point to the _set_ of users who share this point: `{user1, ..., userK}`\n",
    "\n",
    "This way, one can quickly compute how many users share a set of points $(p_1, ..., p_k)$ by computing the size of the _intersection_ $\\bigcap_i \\mathsf{p2u}[p_i]$.\n",
    "\n",
    "The first exercise will be to create function to estimate unicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 _(action required)_** : create functions to extract the `p2u` and `u2p` data structures from the data `CDR`.\n",
    "\n",
    "_hint: `collections.defaultdict` could be useful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_u2p_p2u( CDR ):\n",
    "    '''Returns a tuple (u2p, p2u) computed from the list of points CDR.\n",
    "       u2p maps each userid in CDR to a list of points.\n",
    "       p2u maps each point to a set of userids.'''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sanity check:_ Verify that the code you have written is correct, by checking whether the structures contain compatible information (i.e. every point in `u2p[u]` has `u` in its `p2u` entry, and reciprocally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2p, p2u = compute_u2p_p2u(CDR)\n",
    "\n",
    "error = 0\n",
    "for user in u2p.keys():\n",
    "    for p in u2p[user]:\n",
    "        if user not in p2u[p]:\n",
    "            error += 1\n",
    "            print('Point %d belongs to user %s\\'s u2p, but not reciprocally' % (p, user))\n",
    "for point in p2u.keys():\n",
    "    for u in p2u[point]:\n",
    "        if point not in u2p[u]:\n",
    "            error += 1\n",
    "            print('User %d belongs to point %s\\'s p2u, but not reciprocally' % (u, point))\n",
    "if error:\n",
    "    print('There were %d errors' % error)\n",
    "else:\n",
    "    print('It seems to work fine!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `u2p`, let's confirm we have the right number of users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset contains %d users!' % len(u2p.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 _(action required)_** : using the `u2p` and `p2u` structures, write a function that computes unicity for `k` points.\n",
    "\n",
    "_Hint: use `np.random.choice` to randomly select users and points (with or without replacement?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicity(u2p, p2u, k, nsample=10000):\n",
    "    '''Computes the unicity for k points of the CDR dataset represented by (u2p, p2u).\n",
    "       The argument `nsample` describes the size of the sample of users to use in step 1.\n",
    "       Returns a number between 0 and 1 (the fraction of users unique with k random points).'''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** compute the unicity for $k=2,3,4,5,6$. What do you observe?\n",
    "\n",
    "_Sanity check: you should have $\\varepsilon_2 \\approx 0.66$ and $\\varepsilon_4 \\approx 0.99$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(2,7):\n",
    "    print('k=%d\\te%d = %.3f' % (k,k,unicity(u2p, p2u, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 _(action required)_**: Are the numbers higher or lower than you expected? What if you compare with the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 | Scaling of unicity\n",
    "\n",
    "One of the main criticisms about the unicity metrics is that it's unclear how it evolves with larger populations, and current research has only used \"small\" datasets, subsamples of a larger population. Potentially, the unicity of a large population is very small.\n",
    "\n",
    "In this second exercise, you will use your `unicity` function to compute unicity on populations of growing size, to see how unicity decreases with population size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 _(action required)_**: modify your `unicity` function by adding a `populationsize` argument. The first step of the algorithm should be to sample a set of users of size `populationsize`, and treat it as the population to compute unicity on (in steps 1 and 3).\n",
    "\n",
    "_Hint: you can restrict the population in step 3) by adding the intersection with the population._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicity_population(u2p, p2u, k, populationsize, nsample=10000):\n",
    "    '''Computes the unicity for k points of the a subset of the CDR dataset represented by (u2p, p2u).\n",
    "       The argument `nsample` describes the size of the sample of users to use in step 1.\n",
    "       Returns a number between 0 and 1 (the fraction of users unique with k random points).'''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sanity check: compute the unicity with sample size = 50000 (or 10000, the full population size), and see if you do get approximately the same values._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,7):\n",
    "    print('k=%d\\te%d = %.3f' % (k,k,unicity_population(u2p, p2u, k, len(u2p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Let's compute the unicity of datasets of increasing size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you are using CDR_10k instead, comment out the last point\n",
    "popsize = [100, 5000, 10000, 50000]\n",
    "krange  = range(2, 7)\n",
    "unicvs  = {k:np.zeros((len(popsize),)) for k in krange}\n",
    "\n",
    "for k in krange:\n",
    "    for i, p in enumerate(popsize):\n",
    "        unicvs[k][i] = unicity_population(u2p, p2u, k, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: let's plot the unicity, using `matplotlib`. What do you think of the plot? Does unicity decrease quickly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "colors = {2:'b', 3:'r', 4:'g', 5:'y', 6:'k'}\n",
    "for k in krange:\n",
    "    plt.plot(popsize, unicvs[k], label='k = %d' % k, color=colors[k])\n",
    "\n",
    "plt.title('Unicity as a function of population size')\n",
    "plt.xlabel('population size')\n",
    "plt.ylabel('unicity')\n",
    "plt.ylim([0,1.01])\n",
    "plt.legend(loc=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 _(action required)_**: What does this plot tell you about unicity? Does it seem to decrease fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 | Generalisation\n",
    "\n",
    "In this exercise, you will decrease the precision of the locations and times in the CDR dataset, and see how this affects unicity. For this, you will implement a function `generalise(CDR, spatial_res, temporal_res)` that returns another CDR list, where locations and times have been generalised to some resolution (`spatial_res`, `temporal_res`).\n",
    "\n",
    "**Algorithm**: \n",
    "- To aggregate _spatially_, we will use a _clustering_ algorithm ($k-$means) that aggregates antennas in $k$ clusters. We will define $k$ as the number of antennas divided by `spatial_res`, such that on average, every cluster will contain `spatial_res` antennas.\n",
    "- To aggregate _temporally_, replace each time $t$ (which is an integer corresponding to an hour since the beginning of the dataset collection) by $t~//~\\mathsf{temporal\\_res}$ (where // denotes the integer division). For example, with $\\mathsf{temporal\\_res} = 3$ the timestamps `[1,2,3,4,5,6,7,8,9,10]` would become `[0,0,1,1,1,2,2,2,3,3]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: We provide you with the `spatial_aggregator` function, that returns a dictionary mapping `antenna_id` to the cluster (aggregated antenna) it is part of. The spatial aggregation procedure of the `generalise` function will thus replace antennas by their cluster. We advise you to look at the code, just to see what it does (since these are useful functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clustering provided by the scipy library\n",
    "import scipy.cluster as scc\n",
    "\n",
    "def spatial_aggregator(antennas, spatial_res):\n",
    "    '''Returns a dictionary mapping antennas to cluster (\"super antenna\")'''\n",
    "\n",
    "    # spatial_res is defined as number_of_points / number_of_clusters\n",
    "    n_clusters = antennas.shape[0] // spatial_res\n",
    "\n",
    "    # apply K-means to find the centroids :-)\n",
    "    centroids, quality = scc.vq.kmeans(antennas, n_clusters)\n",
    "\n",
    "    # retrieve the mapping antenna --> centroid (cluster)\n",
    "    clusters, _ = scc.vq.vq(antennas, centroids)\n",
    "\n",
    "    # transform this into a dictionary, to return\n",
    "    return {i:clusters[i] for i in range(antennas.shape[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Plot the clusters resulting from the spatial aggregation procedure to check if you see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spatial_res = 20 # YOUR CHOICE HERE\n",
    "\n",
    "antennas_to_clusters = spatial_aggregator(antennas, spatial_res)\n",
    "\n",
    "clusters = collections.defaultdict(list)\n",
    "\n",
    "for antenna_id, cl in antennas_to_clusters.items():\n",
    "    clusters[cl].append(antenna_id)\n",
    "\n",
    "plt.figure(figsize=(6,8))\n",
    "for cl, antenna_ids in clusters.items():\n",
    "    plt.scatter(antennas[antenna_ids,1], antennas[antenna_ids,0])\n",
    "\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 _(action required)_**: Implement the generalisation algorithm in the `generalise` function, using the `antenna_to_cluster` spatial generalisation provided, and the algorithm we suggested above. Your code must return a new `CDR` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalise(CDR, antennas, spatial_res, temporal_res):\n",
    "    '''Generalises the CDR given as first argument to some spatial\n",
    "       and temporal resolution (both integers > 0).\n",
    "       This uses the `antennas` argument for geographical information on antennas.\n",
    "       Returns a new CDR list, leaving the original untouched'''\n",
    "\n",
    "    # define the mapping to cluster\n",
    "    antenna_to_cluster = spatial_aggregator(antennas, spatial_res)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Choose some values of the resolution, and compute the generalised CDR, as well as the corresponding `u2p` and `p2u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temporal_res = 3\n",
    "spatial_res  = 3\n",
    "\n",
    "CDR_gen = generalise(CDR, antennas, spatial_res, temporal_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u2p_gen, p2u_gen = compute_u2p_p2u(CDR_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: compute the unicity of the dataset after generalisation (using the parameters defined earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,7):\n",
    "    print('k=%d\\te%d = %.3f' % (k,k,unicity(u2p_gen, p2u_gen, k, len(u2p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6 _(action required)_**: So what do you think? Does generalisation really protect you significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 4 | Robustness to noise\n",
    "\n",
    "In this last exercise, you will study how resilient unicity is to uncertainty, in this case spatial uncertainty. For instance, it is common that you know someone's rough location, but not which exact antenna they are using - they could be using any _neighbouring_ antenna. We will assume that the dataset contains the truth, and that the attacker's knowledge is imprecise. Thus, you will have to adapt the way unicity is computed by considering not only users that were at a point `p = (l,t)`, but also at a neighbouring antenna at the same time `p' = (l', t)`.\n",
    "\n",
    "But first, we must define what \"neighbouring antennas\" are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Firstly, we process the `antennas` matrix to obtain its [Delaunay Triangulation](https://en.wikipedia.org/wiki/Delaunay_triangulation). This allows to define \"neighbouring antennas\" for each antenna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.spatial as scs\n",
    "\n",
    "triangulation = scs.Delaunay(antennas)\n",
    "\n",
    "neighbour_antennas = {}\n",
    "indices, indptr = triangulation.vertex_neighbor_vertices\n",
    "for k in range(antennas.shape[0]):\n",
    "    neighbour_antennas[k] = list(indptr[indices[k]:indices[k+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the Delaunay triangulation using Scipy's specific function `delaunay_plot_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scs.delaunay_plot_2d(triangulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of modifying the unicity computation, we can instead adapt the `p2u` structure, so that instead of mapping each point to the set of users in that point, it maps each point to the set of users _in that point and neighbouring points_. This way, we do not need to change the unicity algorithm!\n",
    "\n",
    "**Step 2 _(action required)_**: Create a modified copy of `compute_u2p_p2u`, that returns a modified `p2u` where each point maps to users in this and neighbouring points.\n",
    "\n",
    "_Hint: compute `u2p` and `p2u` normally first, then modify a copy of `p2u`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_u2p_p2u_neighbours( CDR, neighbours ):\n",
    "    '''Returns a tuple (u2p, p2u) computed from the list of points CDR.\n",
    "       `neighbours` is a dictionary mapping a point to a list of its neighbors\n",
    "       u2p maps each userid in CDR to a list of points.\n",
    "       p2u maps each point to a set of userids in the neighbouring points.'''\n",
    "    \n",
    "    u2p, p2u = compute_u2p_p2u( CDR )\n",
    "    p2u_new = dict(p2u) # creates a copy\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return u2p, p2u_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Use this function to compute the unicity of the population with imprecise information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u2p_n, p2u_n = compute_u2p_p2u_neighbours( CDR, neighbour_antennas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,7):\n",
    "    print('k=%d\\te%d = %.3f' % (k,k,unicity(u2p_n, p2u_n, k, len(u2p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 _(no action required)_**: What do you think? Is the unicity attack still successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
